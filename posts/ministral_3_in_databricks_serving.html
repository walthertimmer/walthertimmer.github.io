<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="strict-origin-when-cross-origin">
    <title>ministral 3 in databricks serving | Walther Timmer's meterkast</title>
    <meta name="description" content="Hoe krijg je MVP ministral draaiend in Databricks serving endpoint">
    <meta name="author" content="Walther Timmer">
    <meta property="og:title" content="ministral 3 in databricks serving">
    <meta property="og:description" content="Hoe krijg je MVP ministral draaiend in Databricks serving endpoint">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://walthertimmer.nl/posts/ministral_3_in_databricks_serving.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="ministral 3 in databricks serving">
    <meta name="twitter:description" content="Hoe krijg je MVP ministral draaiend in Databricks serving endpoint">
    <link rel="stylesheet" href="../styles/styles.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KXHQQJVBNV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-KXHQQJVBNV');
</script>

<body>
    <div class="banner"></div>
    <div class="container">
        <h1>Ministral 3 in Databricks serving endpoint</h1>

<p>De Nederlandse overheid vanuit AI visie Rijksoverheid en de diverse visie documenten vanuit de ministeries zoveel mogelijk gebruik te maken van Europese modellen. 
Het is daarom interessant dat er begin december weer een nieuw klein model vanuit Mistral is uitgebracht; Ministral 3 in diverse formaten.</p>

<p>Zie o.a. <a href="https://legal.mistral.ai/ai-governance/models/ministral-3-3b">Ministral 3 - 3b</a> en <a href="https://huggingface.co/collections/mistralai/ministral-3">collectie op huggingface</a>. 
Met vLLM op Kubernetes heb je deze in een aantal commandlines aan de praat. Zie ook de docs van Huggingface. 
Dit los in Databricks als serving endpoint basic aan de praat krijgen is iets meer gehannes.</p>

<h2>Model direct in notebook testen</h2>

<p>Laad het model in GPU.</p>

<pre><code>python
import torch
from transformers import Mistral3ForConditionalGeneration, AutoTokenizer

<h1>Check GPU first</h1>
assert torch.cuda.is_available(), "CUDA not available! Check cluster type and runtime."

<p>model_id = "mistralai/Ministral-3-3B-Instruct-2512-BF16"</p>

<p>tokenizer = AutoTokenizer.from_pretrained(model_id, tokenizer_type="mistral")</p>

<p>model = Mistral3ForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)
</code></pre></p>

<p>Stel een vraag.</p>

<pre><code>python
prompt = "What is Databricks?"

<p>inputs = tokenizer(
    prompt,
    return_tensors="pt"
).to(device)</p>

<p>with torch.no_grad():
    output = model.generate(
        <em></em>inputs,
        max_new_tokens=1024,
        do_sample=True,
        top_p=0.9,
        temperature=0.7
    )</p>

<p>print(tokenizer.decode(output[0], skip_special_tokens=True))
</code></pre></p>

<h2>Registreer een model</h2>

<p>Je kan een eigen PythonModel class maken om met het Mistral model te gebruiken. 
Deze registreer je vervolgens met onderstaande code.</p>

<pre><code>python
ministral_model = Ministral3bModel()

<p>example_input = {
    "prompt": "What is Databricks?",
    "max_new_tokens": 64,
    "temperature": 0.7,
    "top_p": 0.9,
}
example_output = ministral_model.predict(example_input)
signature = infer_signature(example_input, example_output)</p>

<p>model_name = "ministral_3b_instruct"
mlflow.set_registry_uri("databricks-uc")
mlflow.set_experiment("/Shared/ministral-3-3b-instruct-experiment")</p>

<p>with mlflow.start_run() as run:
    mlflow.pyfunc.log_model(
        name="model",
        python_model=ministral_model,
        registered_model_name=f"{catalog_name}.{schema_name}.ministral_3_3b_instruct",
        signature=signature,
        input_example=example_input,
        pip_requirements=[
            "mlflow==3.0.1",
            "huggingface_hub==1.2.3",
            "sentencepiece==0.2.1",
            "tokenizers==0.22.2rc0",
            "cloudpickle==3.1.2",
            "torch==2.7.0",
            "transformers==5.0.0rc0",
            "mistral-common==1.8.6",
            "tiktoken==0.9.0",  # Explicitly add for tekken support
            "accelerate==1.12.0"
        ]
    )
</code></pre></p>

<h2>Test je UC model</h2>

<p>Voor je een serving endpoint aanmaakt is het raadzaam om je model in een notebook sessie te laden en op die manier te testen. 
Dit omdat de serving endpoint container aanmaakactie en serving endpoint aanmaakactie beiden erg traag en ietwat moeilijk te debuggen zijn.</p>

<p>Installeer de benodigde libs.</p>

<pre><code>python
%pip install uv
</code></pre>

<pre><code>python
import mlflow

<p>model_uri = 'models:/m-xxx'</p>

<p>env_file = mlflow.pyfunc.get_model_dependencies(model_uri)
dbutils.widgets.text("env_file", env_file)
%pip install -r $env_file
%restart_python
</code></pre></p>

<p>Test call met eenmalig model laden zodat je hem wat vaker kunt gebruiken met verschillende testcalls zonder telkens opnieuw laadtijd.</p>

<pre><code>python
import mlflow

<p>model_uri = 'models:/m-xxx'
model = mlflow.pyfunc.load_model(model_uri)</p>

<p>INPUT_EXAMPLE = {
    "prompt": "Waarom is een banaan krom?",
    "max_new_tokens": 1000,
    "temperature": 0.7,
    "top_p": 0.9
}</p>

<p>input_data = INPUT_EXAMPLE
predictions = model.predict(input_data)
display(predictions)
</code></pre></p>

<h2>Serving endpoint aanmaken</h2>

<p>Dit op basis van de laatste versie die je in unity catalog hebt geladen.
Indien al beschikbaar gaat de call erroren en daarom separaat een lelijke update in de catch.</p>

<pre><code>python
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import (
    EndpointCoreConfigInput, 
    EndpointTag,
    ServedEntityInput,
    ServingModelWorkloadType
)

<p>w = WorkspaceClient()</p>

<p>endpoint_tags = [
    EndpointTag(key="x", value="y"),
    EndpointTag(key="y", value="x")
]</p>

<p>budget_policy_id = "xxx"
endpoint_name = "ministral-3-3b-instruct"
model_name_uc = f"{catalog_name}.{schema_name}.{model_name}"</p>

<h1>List all versions of the model in Unity Catalog</h1>
model_versions = w.model_versions.list(
    full_name=model_name_uc
)

<h1>Get the latest version (assuming version numbers are integers)</h1>
latest_version = max(
    model_versions,
    key=lambda mv: int(mv.version)
)
print(f"Latest model version: {latest_version.version}")

<h1>Create or update the endpoint</h1>
try:
    print(f"Creating endpoint: {endpoint_name} with budget policy: {budget_policy_id}")
    w.serving_endpoints.create(
        name=endpoint_name,
        config=EndpointCoreConfigInput(
            name=endpoint_name,
            served_entities=[
                ServedEntityInput(
                    entity_name=model_name_uc,
                    entity_version=latest_version.version,
                    workload_size="Small",
                    scale_to_zero_enabled=True,
                    workload_type=ServingModelWorkloadType.GPU_SMALL
                )
            ]
        ),
        budget_policy_id=budget_policy_id,
        route_optimized=True,
        tags=endpoint_tags,
        description="Ministral 3 3B instruct"
    )
    print(f"Created endpoint: {endpoint_name} with budget policy: {budget_policy_id}")
except Exception as e:
    
    print(f"Error with create endpoint: {e}")

<p>if "already exists" in str(e):
        print(f"Endpoint already exists thus updating")</p>

<p>w.serving_endpoints.update_config(
            name=endpoint_name,
            served_entities=[
                ServedEntityInput(
                    entity_name=model_name_uc,
                    entity_version=latest_version.version,
                    workload_size="Small",
                    scale_to_zero_enabled=True,
                    workload_type=ServingModelWorkloadType.GPU_SMALL
                )
            ]
        )
        print(f"Updated endpoint: {endpoint_name}")</p>

<p>print(f"Updating tags for endpoint: {endpoint_name}")
        w.serving_endpoints.patch(
            name=endpoint_name,
            add_tags=endpoint_tags
        )
        print(f"Updated tags for endpoint: {endpoint_name}")
</code></pre></p>

<h2>Query endpoint</h2>

<p>En dan wil je met de vernieuwe serving Databricks endpoints automatisch OAUTH tokens aanmaken om het model aan te roepen. 
Voor het ophalen van Keyvault secrets dien je zelf een functie te schrijven maar dat is niet heel spannend. 
Dat ligt ook deels aan of je Keyvault met RBAC gebruikt of met Access Policies. Databricks lijkt nog steeds de RBAC variant niet te ondersteunen.</p>

<pre><code>python
from databricks.sdk import WorkspaceClient
import databricks.sdk.core as client
from python_module.utils import (
    get_env, 
    get_secret_from_keyvault,
)

<p>host = spark.conf.get("spark.databricks.workspaceUrl")
env = get_env()</p>

<p>clientid = get_secret_from_keyvault(secret_name="sp-servingendpoints-clientid", env=env)
secret = get_secret_from_keyvault(secret_name="sp-servingendpoints-secret", env=env)</p>

<p>endpoint_name = "ministral-3-3b-instruct"</p>

<h1>Initialize Databricks SDK</h1>
c = client.Config(
    host=host,
    client_id=clientid, 
    client_secret=secret   
)
w = WorkspaceClient(
    config = c
)

<p>response = w.serving_endpoints_data_plane.query(
    endpoint_name,
    inputs={
        "prompt": "Explain Databricks in 3 words.",
        "max_new_tokens": 60,
        "temperature": 0.7,
        "top_p": 0.9
    }
)</p>

<p>answer = response.predictions['answer']
print("antwoord: ",answer)
</code></pre></p>

<p>Dit kan je voor alle groottes van de Ministral modellen toepassen al dan niet in een loop mits je je geheugen net leegmaakt na runs zodat je geen OOM errors krijgt.</p>

<h2>Nut kleine modellen</h2>

<p>De grotere fullsize modellen die ergens tussen de 1.2TB en 3TB aan GPU memory zweven zijn interessant en leuk om mee te werken. 
Echter voor veel situaties is dit totaal overkill qua energiegebruik en uitdagingen rondom het mogelijk maken van het seperaat draaien van modellen voor specifieke usecases. 
Iedere ronde GenAI modellen laat weer een vooruitgang zien waarbij de kleinere modellen de oude grote modellen van een paar jaar terug op sommige vlakken weten te evenaren. 
Kleinere modellen zorgen ervoor dat meer business cases van GenAI toepassingen haalbaar zijn en zijn daarom interessant om in de gaten te houden.</p>

<p>Of misschien toch voor 40k aan Mac studios aanschaffen..</p>

<div class="video-container">
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/x4_RsUxRjKU?si=W7R-gT9sVl5fRKID" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

        <a href="../index.html" class="back-link">Terug naar thuis</a>
    </div>
</body>
</html>